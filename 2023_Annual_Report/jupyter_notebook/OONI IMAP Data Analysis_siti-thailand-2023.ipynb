{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41485f29-7287-40d2-a87a-c5daeb84f731",
   "metadata": {},
   "source": [
    "## OONI data analysis case study for IMAP - Thailand\n",
    "\n",
    "### Downloading the data\n",
    "\n",
    "We offer a tool called oonidata (that's currently in BETA), which can be installed by running:\n",
    "```\n",
    "pip install oonidata\n",
    "```\n",
    "\n",
    "To download all OONI data for this example notebook, run the following command:\n",
    "```\n",
    "oonidata sync --country-codes MY TL MM IN VN KH PH TH ID HK --since 2022-07-01 --until 2023-07-01 --output-dir ~/projects/imap/ooni-data/ --test-name webconnectivity\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a96ac23b-e9cd-482c-b598-ba70184eee58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.parser import parse as parse_date\n",
    "from urllib.parse import urlencode, quote, urlparse\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff8b48e-786d-462f-91ed-881f995a9a5f",
   "metadata": {},
   "source": [
    "### OONI Explorer utility functions\n",
    "\n",
    "Below are a couple of useful utility functions when dealing with measurements. They take a dataframe row and return (or print) the OONI Explorer URL. This is useful to get a link to OONI explorer to more easily inspect the raw measurement to better understand what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db0a313f-7410-409a-9059-a6e19bd157a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_explorer_url(e):\n",
    "    query = ''\n",
    "    if 'input' in e.keys() and e['input']:\n",
    "        query = '?input={}'.format(quote(e['input'], safe=''))\n",
    "    return 'https://explorer.ooni.org/measurement/{}{}'.format(e['report_id'], query)\n",
    "    \n",
    "def print_explorer_url(e):\n",
    "    print(get_explorer_url(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f255a254",
   "metadata": {},
   "source": [
    "# Extracting metadata from raw measurements\n",
    "\n",
    "The OONI raw data is very rich, but for most analysis use-cases you just need a subset of the fields or some value that is derived from them.\n",
    "\n",
    "Below are functions that will extract all the metadata we care about from the web_connectivity test.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0ed1fd5-5338-43ec-ae9a-d3dd468a5e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from base64 import b64decode\n",
    "import hashlib\n",
    "import json\n",
    "import re\n",
    "\n",
    "def get_raw_measurement(row):\n",
    "    r = requests.get(\"https://api.ooni.io/api/v1/measurement_meta\", params={\n",
    "        'report_id':row['report_id'],\n",
    "        'input': row['input'],\n",
    "        'full': True\n",
    "    })\n",
    "    j = r.json()\n",
    "    return json.loads(j['raw_measurement'])\n",
    "\n",
    "def get_resolved_ips(msmt):\n",
    "    queries = msmt['test_keys'].get('queries', [])\n",
    "    if not queries:\n",
    "        return ''\n",
    "    answers = queries[0].get('answers', [])\n",
    "    if not answers:\n",
    "        return []\n",
    "    \n",
    "    ip_list = []\n",
    "    for a in answers:\n",
    "        ip = a.get('ipv4', '')\n",
    "        if ip:\n",
    "            ip_list.append(ip)\n",
    "    return ip_list\n",
    "\n",
    "def get_control_failure(msmt):\n",
    "    if 'test_keys' not in msmt:\n",
    "        return 'missing_test_keys'\n",
    "    return msmt['test_keys']['control_failure']\n",
    "\n",
    "def get_test_keys_blocking(msmt):\n",
    "    return str(msmt['test_keys']['blocking'])\n",
    "\n",
    "def get_http_experiment_failure(msmt):\n",
    "    return str(msmt['test_keys']['http_experiment_failure'])\n",
    "\n",
    "def get_resolver_info(msmt):\n",
    "    return {\n",
    "        'resolver_ip': msmt.get('resolver_ip', ''),\n",
    "        'resolver_asn': msmt.get('resolver_asn', ''),\n",
    "        'resolver_network_name': msmt.get('resolver_network_name', '')\n",
    "    }\n",
    "\n",
    "def get_network_events(msmt):\n",
    "    return msmt['test_keys'].get('network_events', [])\n",
    "\n",
    "def get_tcp_connect(msmt):\n",
    "    return msmt['test_keys'].get('tcp_connect', [])\n",
    "\n",
    "def decode_body(body):\n",
    "    if body is None:\n",
    "        return ''\n",
    "    if isinstance(body, dict):\n",
    "        raw_body = b64decode(body['data'])\n",
    "        try:\n",
    "            return raw_body.decode('utf-8')\n",
    "        except:\n",
    "            return raw_body\n",
    "    return body\n",
    "\n",
    "def get_last_response_body(msmt):\n",
    "    try:\n",
    "        # The requests/response list sorts them from the newest to the oldest, \n",
    "        # hence the first item in the list is the last response we received.\n",
    "        body = msmt['test_keys']['requests'][0]['response']['body']\n",
    "        return decode_body(body)\n",
    "    except (KeyError, TypeError, IndexError):\n",
    "        return ''\n",
    "\n",
    "TITLE_REGEXP = re.compile(\"<title.*?>(.*?)</title>\", re.IGNORECASE | re.DOTALL)\n",
    "# Doesn't take into account ordering\n",
    "META_TITLE_REGEXP = re.compile(\"<meta.*?property=\\\"og:title\\\".*?content=\\\"(.*?)\\\"\", re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "def get_http_title(msmt):\n",
    "    body = get_last_response_body(msmt)\n",
    "    # If the body is not a str object, it means it's binary (or an encoding we could not detect). \n",
    "    # No point in trying to extract the title.\n",
    "    # Handling it like this is not very clean or nice.\n",
    "    if not isinstance(body, str):\n",
    "        return ''\n",
    "\n",
    "    m = TITLE_REGEXP.search(body, re.IGNORECASE | re.DOTALL)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    return ''\n",
    "\n",
    "    return extract_title(get_last_response_body(msmt))\n",
    "\n",
    "def get_meta_http_title(msmt):\n",
    "    body = get_last_response_body(msmt)\n",
    "    if not isinstance(body, str):\n",
    "        return ''\n",
    "\n",
    "    m = META_TITLE_REGEXP.search(body, re.IGNORECASE | re.DOTALL)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    return ''\n",
    "\n",
    "def get_http_body_hash(msmt):\n",
    "    body = get_last_response_body(msmt)\n",
    "    if body == '':\n",
    "        return ''\n",
    "    if isinstance(body, str):\n",
    "        # We need the content of the body to be binary.\n",
    "        body = body.encode('utf-8')\n",
    "    return hashlib.md5(body[:2048]).hexdigest()\n",
    "\n",
    "def base_metadata(msmt):\n",
    "    base_keys = [\n",
    "        'input',\n",
    "        'measurement_start_time',\n",
    "        'probe_asn',\n",
    "        'probe_cc',\n",
    "        'probe_network_name',\n",
    "        'report_id',\n",
    "        'resolver_asn',\n",
    "        'resolver_ip',\n",
    "        'resolver_network_name',\n",
    "        'software_name',\n",
    "        'software_version',\n",
    "        'test_name',\n",
    "        'test_runtime',\n",
    "        'test_version'\n",
    "    ]\n",
    "    base_metadata = {}\n",
    "    for k in base_keys:\n",
    "        base_metadata[k] = msmt.get(k, '')\n",
    "    annotations = msmt.pop('annotations')\n",
    "    base_metadata['network_type'] = annotations.get('network_type', 'unknown')\n",
    "    base_metadata['origin'] = annotations.get('origin', 'unknown')\n",
    "    base_metadata['platform'] = annotations.get('platform', 'unknown')\n",
    "    return base_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77b2edfd-8746-4bfb-b03f-f78bf33fc178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_measurement_meta(msmt):\n",
    "    m = base_metadata(msmt)\n",
    "    m.update(get_resolver_info(msmt))\n",
    "    m.update({\n",
    "        'dns_resolved_ips': get_resolved_ips(msmt),\n",
    "        'network_events': get_network_events(msmt),\n",
    "        'control_failure': get_control_failure(msmt),\n",
    "        'control_measurement': msmt['test_keys']['control'],\n",
    "        'blocking': get_test_keys_blocking(msmt),\n",
    "        'http_experiment_failure': get_http_experiment_failure(msmt),\n",
    "        'dns_experiment_failure': str(msmt['test_keys']['dns_experiment_failure']),\n",
    "        'http_title': get_http_title(msmt),\n",
    "        'http_meta_title': get_meta_http_title(msmt),\n",
    "        'http_body_md5': get_http_body_hash(msmt),\n",
    "        'tcp_connect': get_tcp_connect(msmt),\n",
    "    })\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1851ba3-b009-46fc-b3c7-09aac2d74254",
   "metadata": {},
   "source": [
    "### Parsing raw files on disk, filtering and transforming them\n",
    "\n",
    "Below are functions that will list the files on disk, given a search query, and return an iterator of the raw measurement dict.\n",
    "\n",
    "These functions are then called by either `msmt_to_csv` or `get_msmt_df`, which write the processed data to a CSV file or load it in memory as a pandas DataFrame respectively.\n",
    "\n",
    "It's generally recommended, when you are dealing with very large datasets, to write the minimised form of the data to a file on disk so that you don't have to re-parse everything if your notebook crashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75ba3baf-0e09-44e6-beb4-334867885d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import gzip\n",
    "import ujson\n",
    "\n",
    "data_dir = Path(\"/home/sitinurliza/projects/imap/ooni-data\")\n",
    "\n",
    "def iter_msmts(fp):\n",
    "    with gzip.open(fp) as in_file:\n",
    "        yield from [ujson.loads(line) for line in in_file]\n",
    "            \n",
    "def iter_jsonl_paths(query):\n",
    "    for p in data_dir.glob('*/*/*/*'):\n",
    "        ts, cc, tn = p.name.split('_')\n",
    "        tn = tn.split('.')[0]\n",
    "        ts = datetime.strptime(ts, '%Y%m%d%H')\n",
    "        if query.get('probe_cc') and cc != query['probe_cc']:\n",
    "            continue\n",
    "        if query.get('test_name') and tn != query['test_name'].replace('_', ''):\n",
    "            continue\n",
    "        if query.get('since') and parse_date(query['since']) >= ts:\n",
    "            continue\n",
    "        if query.get('until') and parse_date(query['until']) <= ts:\n",
    "            continue\n",
    "        yield p\n",
    "        \n",
    "def iter_raw_measurements(query):\n",
    "    path_list = list(iter_jsonl_paths(query))\n",
    "    print(f\"processing {len(path_list)}\")\n",
    "    for fp in tqdm(path_list):\n",
    "        for msmt in iter_msmts(fp):\n",
    "            if query.get('probe_asn') and msmt['probe_asn'] != query['probe_asn']:\n",
    "                continue\n",
    "            if query.get('domain'):\n",
    "                domain = urlparse(msmt['input']).netloc\n",
    "                if domain != query['domain']:\n",
    "                    continue\n",
    "            yield msmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6439fc37-2715-4cc1-9b50-a4754aebf955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def msmt_to_csv(query, output_file=\"output.csv\"):\n",
    "    with open(output_file, 'w') as output_file:\n",
    "        csv_writer = None\n",
    "        for msmt in iter_raw_measurements(query):\n",
    "            msmt_meta = get_measurement_meta(msmt)\n",
    "            if csv_writer is None:\n",
    "                fieldnames = msmt_meta.keys()\n",
    "                csv_writer = csv.DictWriter(output_file, fieldnames=fieldnames)\n",
    "                csv_writer.writeheader()\n",
    "            csv_writer.writerow(msmt_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "898e8ebf-bce6-4a7e-905a-0563460b539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_msmt_df(query):\n",
    "    msmt_list = []\n",
    "    for msmt in iter_raw_measurements(query):\n",
    "        mdf = pd.DataFrame([get_measurement_meta(msmt)])\n",
    "        msmt_list.append(mdf)\n",
    "    return pd.concat(msmt_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbaf33a-5686-4c16-8c9c-380fa55f0302",
   "metadata": {},
   "source": [
    "Here we do the actual conversion to CSV. Edit the dates and country codes accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5546b9-d164-4f14-a115-ff14cfe675b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "msmt_to_csv({\n",
    "    'since': '2022-07-01',\n",
    "    'until': '2022-12-31',\n",
    "    'probe_cc': 'TH',\n",
    "    'test_name': 'web_connectivity'\n",
    "}, output_file=\"ooni-data-wc-thailand-2023-1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d406f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "msmt_to_csv({\n",
    "    'since': '2023-01-01',\n",
    "    'until': '2023-05-31',\n",
    "    'probe_cc': 'TH',\n",
    "    'test_name': 'web_connectivity'\n",
    "}, output_file=\"ooni-data-wc-thailand-2023-2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bda37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "msmt_to_csv({\n",
    "    'since': '2023-06-01',\n",
    "    'until': '2023-07-01',\n",
    "    'probe_cc': 'TH',\n",
    "    'test_name': 'web_connectivity'\n",
    "}, output_file=\"ooni-data-wc-thailand-2023-3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb1e94e-746b-42cd-898f-6f46122e65d1",
   "metadata": {},
   "source": [
    "We then load the CSV file in memory as a pandas dataframe for more analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24bf8f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_th_1 = pd.read_csv('ooni-data-wc-thailand-2023-1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99c48f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_th_2 = pd.read_csv('ooni-data-wc-thailand-2023-2.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f297a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_th_3 = pd.read_csv('ooni-data-wc-thailand-2023-3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f68b67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_th = pd.concat([df_th_1,df_th_2,df_th_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7c05cfc-c6bc-48d3-b4d0-7ec3f85eecef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4646454"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "492cf489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input',\n",
       " 'measurement_start_time',\n",
       " 'probe_asn',\n",
       " 'probe_cc',\n",
       " 'probe_network_name',\n",
       " 'report_id',\n",
       " 'resolver_asn',\n",
       " 'resolver_ip',\n",
       " 'resolver_network_name',\n",
       " 'software_name',\n",
       " 'software_version',\n",
       " 'test_name',\n",
       " 'test_runtime',\n",
       " 'test_version',\n",
       " 'network_type',\n",
       " 'origin',\n",
       " 'platform',\n",
       " 'dns_resolved_ips',\n",
       " 'network_events',\n",
       " 'control_failure',\n",
       " 'control_measurement',\n",
       " 'blocking',\n",
       " 'http_experiment_failure',\n",
       " 'dns_experiment_failure',\n",
       " 'http_title',\n",
       " 'http_meta_title',\n",
       " 'http_body_md5',\n",
       " 'tcp_connect']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_th)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4ae6ad",
   "metadata": {},
   "source": [
    "### Adding columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9616a5be-8915-45af-93dd-4ad6e0d01c82",
   "metadata": {},
   "source": [
    "When dealing with websites, we generally care to look at data from a domain centric perspective. This allows us to group together URLs that are of the same domain, but that have different paths.\n",
    "\n",
    "Since the raw dataset doesn't include the `domain` we add this column here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0adbc67f-adf9-4d7b-aa47-93b91b696b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 4646454/4646454 [01:02<00:00, 74202.90it/s]\n"
     ]
    }
   ],
   "source": [
    "def parse_domain(url):\n",
    "    try:\n",
    "        return urlparse(url).netloc\n",
    "    except:\n",
    "        print(f'invalid url {url}')\n",
    "        return ''\n",
    "df_th['domain'] = df_th['input'].progress_apply(parse_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c15eadb9-8149-4d80-b011-f9c72a59dbb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.50961035862565"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_th.memory_usage(deep=True).sum()/1024**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "deccef8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df_th['explorer_url'] = \"https://explorer.ooni.org/measurement/\" + df_th['report_id'] + \"?input=\" + df_th['input']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e2c26e",
   "metadata": {},
   "source": [
    "### Listing out probe_asn names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e377cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_th[['probe_asn', 'probe_network_name']].drop_duplicates(subset=['probe_asn']).to_csv('probe_names_TH_2023.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf49c12-b6c0-4c0c-af15-9872f28fe712",
   "metadata": {},
   "source": [
    "### Hunting for blocking fingerprints\n",
    "\n",
    "We can have a very high confidence that the blocking is intentional (and not caused by transient network failures), when it fits in the following classes:\n",
    "- DNS level interference\n",
    "- HTTP level intereference\n",
    "- TLS MITM\n",
    "\n",
    "\n",
    "The first two classes, though, are susceptive to false positives, because sometimes the IP returned in a DNS query can differ based on the geographical location (think CDNs) and sometimes the content of a webpage can also vary from request to request (think the homepage of a news site).\n",
    "\n",
    "On the other hand, once we find a blocking fingerprint, we can with great confidence claim that access to that particular site is being restricted. For example we might notice that when a site is blocked on a particular network, the DNS query always returns a given IP address or we might know that the HTTP title for a blockpage is always \"Access to this website is denied\".\n",
    "\n",
    "Our goal now to come up with some heuristics that will allow us to, in a way, hunt for these blockpage fingerprints in the big dataset that we have available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce9e1ba-6888-44a4-bc74-3daaf697f89e",
   "metadata": {},
   "source": [
    "### Same title, but different page\n",
    "\n",
    "One heuristic which we can apply to spotting blockpages, is that we can say that a web page that looks exactly the same for many different sites. Based on this fairly simple intuition, we can look for blockpage fingerprints by just counting for the number of domains that share the same HTTP title tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b97c56c-2da8-4987-aabd-191f6fb5003a",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_domain_count = df_th[\n",
    "    df_th['blocking'] == 'http-diff'\n",
    "].groupby('http_title')['domain'].nunique().sort_values().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4881770f-2b65-4574-88c4-ac6611ee02cf",
   "metadata": {},
   "source": [
    "As we can see in the breakdown below, all these blockpage fingerprints look fairly suspicious and are quite likely to be an indication of blocking. Some of them, however, might be signs of server-side blocking (ex. Geoblocking or DDOS prevention). This is why it's best, to obtain a high degree of accuracy, to investigate these manually and add them to a fingerprint database.\n",
    "\n",
    "This is a shared effort amonst censorship research projects, for example you can find a repo of known blocking fingerprints maintained by the CitizenLab here: https://github.com/citizenlab/filtering-annotations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf95466-f879-4187-a670-8f38ab86c95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_domain_count[\n",
    "    title_domain_count['domain'] > 8\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d666d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "http_title_suspicious = title_domain_count[title_domain_count['domain'] > 8]['http_title'].values.tolist()\n",
    "http_title_suspicious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2cef62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_th[df_th['http_title'].isin(http_title_suspicious)].drop_duplicates(subset=['http_title']).to_csv('http_title_suspicious_TH_2023.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd33a5d0-bed6-4d8f-a6e1-9cf1337785b3",
   "metadata": {},
   "source": [
    "Once we have confirmed that a fingerprint is known to implement blocking, we can use it to which domains are being restricted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6290b7e9",
   "metadata": {},
   "source": [
    "In the case of Thailand, it was found that the blocking was using http but only indicated in the http body. Hence we group the domains by their `http_body_md5` to look for signs of blocking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7ff043",
   "metadata": {},
   "outputs": [],
   "source": [
    "body_domain_count = df_th[\n",
    "    df_th['blocking'] == 'http-diff'\n",
    "].groupby('http_body_md5')['domain'].nunique().sort_values().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7229262",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "body_domain_count[\n",
    "    body_domain_count['domain'] > 8\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f815f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "http_body_suspicious = body_domain_count[body_domain_count['domain'] > 8]['http_body_md5'].values.tolist()\n",
    "http_body_suspicious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abac73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_th[df_th['http_body_md5'].isin(http_body_suspicious)].drop_duplicates(subset=['http_body_md5']).to_csv('http_body_suspicious_TH_2023.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946144f0-43c1-44c8-987e-9495b96f578f",
   "metadata": {},
   "source": [
    "### DNS level interference\n",
    "\n",
    "We can use a similar heuristics for DNS level interference. The assumption is the same, when we see one IP being mapped to multiple hostnames, it's an indication of it potentially being an IP used to implement blocking.\n",
    "\n",
    "In this case, we need to be careful of false positives that might be caused by the use of CDNs, as these will be hosting multiple sites. In the sections below we can see what techniques we can adopt to reduce these false positives further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe85118-9a33-4acb-b945-53403165cbe5",
   "metadata": {},
   "source": [
    "We are going to make use of a IP to ASN database for some of our heuristics. In particular we are going to download the one from db-ip, which has a fairly permissive license and is compatible with the maxmind database format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffd5107-536f-4603-8f82-1203e6ccf2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O https://download.db-ip.com/free/dbip-asn-lite-2022-06.mmdb.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eb1a3a-2836-4164-b6d0-9c96c175a376",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gunzip dbip-asn-lite-2023-06.mmdb.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38f87a59-a8c7-4c0c-b0b4-2a228a8af3c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import maxminddb\n",
    "\n",
    "asn_db_path = 'dbip-asn-lite-2023-06.mmdb'\n",
    "def lookup_asn(ip):\n",
    "    with maxminddb.open_database(asn_db_path) as reader:\n",
    "        try:\n",
    "            return reader.get(ip)\n",
    "        # Probably not an IP\n",
    "        except ValueError:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b35948-b9e6-4412-ad56-4859b8ee8f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "dns_resp_sorted = df_th[\n",
    "    df_th['blocking'] == 'dns'\n",
    "].groupby('dns_resolved_ips')['domain'].nunique().sort_values().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35cb825-4342-476b-aa2d-1721f9ccd7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dns_resp_sorted[\n",
    "    dns_resp_sorted['domain'] > 2\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e494cca9-9041-4d07-8e7d-fe1da135ed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dns_resp_sorted[\n",
    "    dns_resp_sorted['domain'] > 10\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4157ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dns_suspicious = dns_resp_sorted[dns_resp_sorted['domain'] > 10]['dns_resolved_ips'].values.tolist()\n",
    "dns_suspicious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d555280e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_th[df_th['dns_resolved_ips'].isin(dns_suspicious)].drop_duplicates(subset=['dns_resolved_ips']).to_csv('dns_suspicious_TH_2023.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b97d53-57bc-4862-aeff-3ce8c7f20c30",
   "metadata": {},
   "source": [
    "### DNS inconsistency false positive removal\n",
    "\n",
    "To understand if what we are looking at is a real blocking IP or not, we can use the following heuristics:\n",
    "\n",
    "1. Does the IP in question have a PTR record pointing to something that looks like a blockpage (ex. a hostname that is related to the ISP)\n",
    "2. What information can we get about the IP by doing a whois lookup\n",
    "3. Is the ASN of the IP the same as the network where the measurement was collected\n",
    "4. Do we get a valid TLS certificate for one of the domains in question when doing a TLS handshake and specifying the SNI\n",
    "\n",
    "Using these 4 conditions, we are generally able to understand if it's in fact a blocking IP or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87f65c8-b17f-49ec-9ab2-a8d497cc3e1d",
   "metadata": {},
   "source": [
    "### Categorizing IPs into confirmed positives and false positives, as well as http titles that are confirmed positives..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77d8a03e-1a05-4b2d-94bd-71c9a8c61dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed_ips = [\n",
    "    '125.26.170.3'\n",
    "]\n",
    "\n",
    "false_positive_ips = [\n",
    "    '52.15.96.207',\n",
    "    '146.112.197.76',\n",
    "    '146.112.197.80',\n",
    "    '146.112.61.106'\n",
    "]\n",
    "\n",
    "confirmed_titles = [\n",
    "    'Tcsd.info'\n",
    "]\n",
    "\n",
    "confirmed_bodies = [\n",
    "    'd21932611e57716abd531bf9b865a684',\n",
    "    '39687c596d2a53237334159b16fdd6c8',\n",
    "    '045184cc004470459e612bcd16157ff3',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "907d1dbc-b9f4-42ae-936a-1e7067506baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ip_map = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5882b3a1-29c3-4519-a517-d232f7e39a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import certifi\n",
    "import ssl\n",
    "import socket\n",
    "\n",
    "def is_tls_valid(ip, hostname):\n",
    "    if len(df_th[\n",
    "        (df_th['dns_resolved_ips'].str.contains(ip, na=False))\n",
    "        & (df_th['domain'] == hostname)\n",
    "        & (df_th['input'].str.startswith('https'))\n",
    "        & (df_th['http_experiment_failure'] == 'None')\n",
    "    ]) > 0:\n",
    "        return True\n",
    "\n",
    "    context = ssl.SSLContext(ssl.PROTOCOL_TLS_CLIENT)\n",
    "    context.load_verify_locations(certifi.where())\n",
    "\n",
    "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM, 0) as sock:\n",
    "        sock.settimeout(1)\n",
    "        with context.wrap_socket(sock, server_hostname=hostname) as conn:\n",
    "            try:\n",
    "                conn.connect((ip, 443))\n",
    "            # TODO: do we care to distinguish these values?\n",
    "            except ssl.SSLCertVerificationError:\n",
    "                return False\n",
    "            except ssl.SSLError:\n",
    "                return False\n",
    "            except socket.timeout:\n",
    "                return False\n",
    "            except socket.error:\n",
    "                return False\n",
    "            except:\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "def is_tls_valid_with_cache(ip, hostname):\n",
    "    key = f\"{ip}{hostname}\"\n",
    "    if key in valid_ip_map:\n",
    "        return valid_ip_map[key]\n",
    "    valid_ip_map[key] = is_tls_valid(ip, hostname)\n",
    "    return valid_ip_map[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba0d045-31c2-4766-8b19-544860051169",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "We can then proceed to automating the detection on the full dataset. Our goal is that of recomputing the `blocking` feature for each individual measurement based on our improved heuristics.\n",
    "\n",
    "In addition to the previously discussed DNS and HTTP based blocking, we are going to additionally classify blocking that happens at different layers of the network stack.\n",
    "\n",
    "Specifically, we are going to be using the following identifiers for the various ways in which blocking might occur:\n",
    "\n",
    "#### DNS\n",
    "* dns.confirmed - one of the returned IPs matches an IP known to be used to implement blocking\n",
    "* dns.no_ipv4 - no IPv4 address was returned\n",
    "* dns.bogon - a bogon IP address was returned\n",
    "* dns.nxdomain - we got an NXDOMAIN response from the probe, but we got a valid response from the control vantage point\n",
    "* dns.inconsistent - our DNS consistency heuristics determined the returned IP to be inconsistent\n",
    "\n",
    "#### HTTP\n",
    "\n",
    "These are all blocking types related to plaintext HTTP requests:\n",
    "\n",
    "* http.confirmed - the returned page is a known blockpages\n",
    "* http.http_diff - the page doesn't match based on our page consistency heuristics\n",
    "* http.connection_reset - we got a connection reset to a plaintext HTTP request\n",
    "* http.connection_closed - the connection was closed before all data was transmitted\n",
    "* http.connection_timeout - the connection timed out before we could retrieve all the data \n",
    "* http.generic_failure - this is an generic error from legacy OONI probes\n",
    "\n",
    "#### TLS\n",
    "\n",
    "These are all blocking types related to TLS:\n",
    "\n",
    "* tls.connection_reset - a reset packet was seen after the client sent the ClientHello packet\n",
    "* tls.connection_closed - the connection was closed after the ClientHello\n",
    "* tls.connection_timeout - the connection timed out after the ClientHello\n",
    "    * All of the above can also have the `_after_hello` suffix, indicating that the event happened after the client sent the ClienHello packet\n",
    "* tls.mitm - The DNS is consistent, but the TLS certificate validation failed. This suggest a TLS man-in-the-middle\n",
    "* tls.generic_failure - generic error from legacy OONI probes\n",
    "\n",
    "#### TCP/IP\n",
    "\n",
    "This is when blocking is implemented by targeting the IP address of the host:\n",
    "\n",
    "* tcp.connection_reset - the TCP connect test failed due to a reset packet\n",
    "* tcp.connection_timeout - the TCP connect test failed with a timeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "477cde66-7af8-446d-afaa-5e859fc74e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "import ipaddress\n",
    "\n",
    "def normalize_failure(failure_str):\n",
    "    if \"An existing connection was forcibly closed by the remote host\" in failure_str:\n",
    "        return \"connection_reset\"\n",
    "    if \"No address associated with hostname\" in failure_str:\n",
    "        return \"dns_nxdomain_error\"\n",
    "    return failure_str\n",
    "\n",
    "def is_dns_asns_consistent(dns_resolved_ips, control_measurement, row):\n",
    "    try:\n",
    "        control_addrs = control_measurement['dns']['addrs']\n",
    "        if not control_addrs:\n",
    "            return False\n",
    "        control_asns = set(list(map(lambda e: e['autonomous_system_number'], \n",
    "                           filter(lambda e: e != None, map(lookup_asn, control_addrs)))))\n",
    "        exp_asns = set(list(map(lambda e: e['autonomous_system_number'], \n",
    "                           filter(lambda e: e != None, map(lookup_asn, dns_resolved_ips)))))\n",
    "        if exp_asns.intersection(control_asns):\n",
    "            return True\n",
    "    except KeyError:\n",
    "        # Missing control measurement\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "bogon_ipv4_ranges = [\n",
    "    ipaddress.ip_network(\"0.0.0.0/8\"), # \"This\" network\n",
    "    ipaddress.ip_network(\"10.0.0.0/8\"), # Private-use networks\n",
    "    ipaddress.ip_network(\"100.64.0.0/10\"), # Carrier-grade NAT\n",
    "    ipaddress.ip_network(\"127.0.0.0/8\"), # Loopback\n",
    "    ipaddress.ip_network(\"127.0.53.53\"), # Name collision occurrence\n",
    "    ipaddress.ip_network(\"169.254.0.0/16\"), # Link local\n",
    "    ipaddress.ip_network(\"172.16.0.0/12\"), # Private-use networks\n",
    "    ipaddress.ip_network(\"192.0.0.0/24\"), # IETF protocol assignments\n",
    "    ipaddress.ip_network(\"192.0.2.0/24\"), # TEST-NET-1\n",
    "    ipaddress.ip_network(\"192.168.0.0/16\"), # Private-use networks\n",
    "    ipaddress.ip_network(\"198.18.0.0/15\"), # Network interconnect device benchmark testing\n",
    "    ipaddress.ip_network(\"198.51.100.0/24\"), # TEST-NET-2\n",
    "    ipaddress.ip_network(\"203.0.113.0/24\"), # TEST-NET-3\n",
    "    ipaddress.ip_network(\"224.0.0.0/4\"), # Multicast\n",
    "    ipaddress.ip_network(\"240.0.0.0/4\"), # Reserved for future use\n",
    "    ipaddress.ip_network(\"255.255.255.255/32\"), # Limited broadcast\n",
    "]\n",
    "def is_dns_bogon(dns_resolved_ips):\n",
    "    for ip in dns_resolved_ips:\n",
    "        ipv4addr = ipaddress.IPv4Address(ip)\n",
    "        if any([ipv4addr in ip_range for ip_range in bogon_ipv4_ranges]):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_dns_tls_consistent(dns_resolved_ips, row):\n",
    "    # If it's a HTTPs site and we didn't get a TLS error, we can assume the IPs are valid\n",
    "    if row['input'].startswith('https://') and row['http_experiment_failure'] == 'None':\n",
    "        return False\n",
    "    \n",
    "    for ip in dns_resolved_ips:\n",
    "        domain = urlparse(row['input']).netloc\n",
    "        if is_tls_valid_with_cache(ip, domain):\n",
    "            # We consider the first hit to be enough to consider it consistent\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_dns_false_positive(dns_resolved_ips):\n",
    "    for ip in dns_resolved_ips:\n",
    "        if ip in false_positive_ips:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def recompute_blocking(row):\n",
    "    if not isinstance(row['input'], str):\n",
    "        return 'invalid'\n",
    "    \n",
    "    try:\n",
    "        dns_resolved_ips = literal_eval(row['dns_resolved_ips'])\n",
    "    except:\n",
    "        dns_resolved_ips = []\n",
    "\n",
    "    blocking = row['blocking']\n",
    "    for ip in dns_resolved_ips:\n",
    "        if ip in confirmed_ips:\n",
    "            return 'dns.confirmed'\n",
    "        \n",
    "    # This is a special case for when we got no ipv4 addresses and the network doesn't support ipv6\n",
    "    if len(dns_resolved_ips) == 0 and row['http_experiment_failure'] == 'network_unreachable':\n",
    "        return 'dns.no_ipv4'\n",
    "    \n",
    "    if is_dns_bogon(dns_resolved_ips):\n",
    "        return 'dns.bogon'\n",
    "\n",
    "    try:\n",
    "        control_measurement = literal_eval(row['control_measurement'])\n",
    "    except:\n",
    "        return 'invalid'\n",
    "    if not control_measurement:\n",
    "        return 'invalid'\n",
    "    \n",
    "    if control_measurement['http_request']['failure'] != None:\n",
    "        return 'invalid'\n",
    "\n",
    "    if (normalize_failure(row['dns_experiment_failure']) == 'dns_nxdomain_error' and \n",
    "            control_measurement.get('http_request', {}).get('failure', '') != 'dns_lookup_error'):\n",
    "        return 'dns.nxdomain'\n",
    "\n",
    "    if (\n",
    "        not (row['input'].startswith('https://') and row['http_experiment_failure'] == 'None') \n",
    "        and not is_dns_false_positive(dns_resolved_ips) \n",
    "        and not is_dns_asns_consistent(dns_resolved_ips, control_measurement, row)\n",
    "        #and not is_dns_tls_consistent(dns_resolved_ips, row)\n",
    "    ):\n",
    "        return 'dns.inconsistent'\n",
    "\n",
    "    # If we got down to here, it means that DNS is consistent    \n",
    "    if row['http_title'] in confirmed_titles:\n",
    "        return 'http.title.confirmed'\n",
    "    \n",
    "    if row['http_body_md5'] in confirmed_bodies:\n",
    "        return 'http.body.confirmed'\n",
    "    \n",
    "    if blocking == 'http-diff' and row['input'].startswith('http://'):\n",
    "        return 'http.http_diff'\n",
    "    \n",
    "    if row['http_experiment_failure'] != 'None':\n",
    "        tcp_connect_list = literal_eval(row['tcp_connect'])\n",
    "        for conn in tcp_connect_list:\n",
    "            if conn['status']['failure'] == 'connection_reset':\n",
    "                return 'tcp.connection_reset'\n",
    "            elif conn['status']['failure'] == 'generic_timeout_error':\n",
    "                return 'tcp.connection_timeout'\n",
    "    \n",
    "    # We compute TLS level anomalies this using the network_events\n",
    "    tls_handshake_started = False\n",
    "    try:\n",
    "        network_events = literal_eval(row['network_events'])\n",
    "    except:\n",
    "        network_events = []\n",
    "    if network_events:\n",
    "        for idx, network_event in enumerate(network_events):\n",
    "            global write_operations\n",
    "            global read_operations\n",
    "            \n",
    "            if network_event['operation'] == 'write':\n",
    "                write_operations += 1\n",
    "            if network_event['operation'] == 'read':\n",
    "                read_operations += 1\n",
    "\n",
    "            if tls_handshake_started and network_event['failure']:\n",
    "                # We are guaranteed to not be out of bounds due to the tls_handshake_started flag\n",
    "                prev_operation = network_events[idx-1]\n",
    "                \n",
    "                suffix = ''\n",
    "                if normalize_failure(network_event['failure']) == 'connection_reset':\n",
    "                    return f'tls.connection_reset{suffix}'\n",
    "                elif normalize_failure(network_event['failure']) == 'eof_error':\n",
    "                    return f'tls.connection_closed{suffix}'\n",
    "                elif normalize_failure(network_event['failure']) == 'generic_timeout_error':\n",
    "                    return f'tls.connection_timeout{suffix}'\n",
    "                if write_operations > 1:\n",
    "                    suffix = f'_after_hello'\n",
    "\n",
    "            if network_event['operation'] == 'tls_handshake_start':\n",
    "                tls_handshake_started = True\n",
    "                write_operations = 0\n",
    "                read_operations = 0\n",
    "            if network_event['operation'] == 'tls_handshake_done':\n",
    "                tls_handshake_started = False\n",
    "\n",
    "    # If we got down to here, it means the DNS consistency checks have passed\n",
    "    # For the http related failures, if we are spotting them here, it means the test most likely doesn't support the \n",
    "    # new network_events keys, and therefore the results are a bit less accurate.\n",
    "    # This should ideally be indicated via a lower confidence value.\n",
    "    if normalize_failure(row['http_experiment_failure']) == 'connection_reset':\n",
    "        if row['input'].startswith('https://'):\n",
    "            return 'tls.connection_reset'\n",
    "        else:\n",
    "            return 'http.connection_reset'\n",
    "    elif normalize_failure(row['http_experiment_failure']) == 'eof_error':\n",
    "        if row['input'].startswith('https://'):\n",
    "            return 'tls.connection_closed'\n",
    "        else:\n",
    "            return 'http.connection_closed'\n",
    "    elif normalize_failure(row['http_experiment_failure']) == 'generic_timeout_error':\n",
    "        if row['input'].startswith('https://'):\n",
    "            return 'tls.connection_timeout'\n",
    "        else:\n",
    "            return 'http.connection_timeout'\n",
    "    # It's not just using DNS to point us to an IP that serves a blockpage and it's a TLS MITM\n",
    "    elif row['input'].startswith('https://') and row['http_experiment_failure'].startswith('ssl_'):\n",
    "        return 'tls.mitm'\n",
    "    \n",
    "    # We map unknown_failures to invalid measurements\n",
    "    elif row['http_experiment_failure'].startswith('unknown_failure'):\n",
    "        return 'invalid'\n",
    "    \n",
    "    # All unmapped errors go into a generic failure pool\n",
    "    elif row['http_experiment_failure'] != 'None':\n",
    "        if row['input'].startswith('https://'):\n",
    "            return 'tls.generic_failure'\n",
    "        else:\n",
    "            return 'http.generic_failure'\n",
    "    \n",
    "    return 'ok'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24c3560c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_th_a = df_th[0:int(len(df_th)/2)]\n",
    "\n",
    "if (len(df_th) % 2) == 0:\n",
    "    df_th_b = df_th[int(len(df_th)/2):]\n",
    "else:\n",
    "    df_th_b = df_th[int(len(df_th)):-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ae87631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2323227"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_th_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9404b7f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2323227"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_th_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d8719d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4646454"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_th_a) + len (df_th_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c72870a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 2323227/2323227 [45:07<00:00, 858.22it/s]\n",
      "/tmp/ipykernel_2024102/3880095253.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_th_a['blocking_recalc'] = df_th_a.progress_apply(recompute_blocking, axis=1)\n"
     ]
    }
   ],
   "source": [
    "df_th_a['blocking_recalc'] = df_th_a.progress_apply(recompute_blocking, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7ea9ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 2323227/2323227 [46:32<00:00, 831.99it/s]\n",
      "/tmp/ipykernel_2024102/52728396.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_th_b['blocking_recalc'] = df_th_b.progress_apply(recompute_blocking, axis=1)\n"
     ]
    }
   ],
   "source": [
    "df_th_b['blocking_recalc'] = df_th_b.progress_apply(recompute_blocking, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0fd72d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_th_new = pd.concat([df_th_a,df_th_b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed0b8ebc-063f-49bf-98ed-e2baadddf55b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ok', 'dns.confirmed', 'dns.inconsistent', 'invalid',\n",
       "       'http.body.confirmed', 'http.http_diff', 'tls.connection_reset',\n",
       "       'http.title.confirmed', 'dns.bogon', 'tls.connection_closed',\n",
       "       'http.generic_failure', 'tls.mitm', 'tls.connection_timeout',\n",
       "       'http.connection_timeout', 'http.connection_reset', 'dns.nxdomain',\n",
       "       'tcp.connection_timeout', 'tls.generic_failure', 'dns.no_ipv4',\n",
       "       'http.connection_closed'], dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_th_new['blocking_recalc'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "285f43ee-1abf-4f96-92c9-b9026fb0f55f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"['65.9.181.6', '65.9.181.35', '65.9.181.23', '65.9.181.79']\",\n",
       "       \"['125.252.242.195']\", \"['104.76.138.9']\", ...,\n",
       "       \"['111.223.36.21', '203.159.100.10', '111.223.36.22', '103.253.132.21', '103.253.132.22', '203.159.100.2']\",\n",
       "       \"['199.59.148.209']\",\n",
       "       \"['203.159.100.10', '111.223.36.22', '103.253.132.21', '103.253.132.22', '203.159.100.2']\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_th_new[\n",
    "    df_th_new['blocking_recalc'] == 'dns.inconsistent'\n",
    "]['dns_resolved_ips'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0a42c9-992a-4482-9724-d7f2edf1f2fb",
   "metadata": {},
   "source": [
    "Let's see on how many networks we were able to confirm the blocking of sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ebbd857c-d768-4d9d-906b-d20ba42c4455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AS17552', 'AS131445', 'AS133481', 'AS45758', 'AS23969',\n",
       "       'AS132061', 'AS132280', 'AS7470', 'AS24378', 'AS136538', 'AS45430',\n",
       "       'AS55423', 'AS45629', 'AS132618', 'AS38794', 'AS45806', 'AS131090',\n",
       "       'AS4750', 'AS45458', 'AS63940'], dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_th_new[\n",
    "    df_th_new['blocking_recalc'] == 'dns.confirmed'\n",
    "]['probe_asn'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe1c2d1c-1875-486f-b248-de47ba842d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AS17552', 'AS7470', 'AS132061', 'AS45758', 'AS4762', 'AS132618',\n",
       "       'AS131090', 'AS24378', 'AS131445', 'AS17479'], dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_th_new[\n",
    "    df_th_new['blocking_recalc'] == 'http.title.confirmed'\n",
    "]['probe_asn'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "af95a1e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AS23969', 'AS17552', 'AS131445', 'AS45629', 'AS133481', 'AS45758',\n",
       "       'AS45430', 'AS7470', 'AS132618', 'AS55423', 'AS4762', 'AS132061',\n",
       "       'AS136538', 'AS56277', 'AS24378', 'AS45458', 'AS131090', 'AS4750',\n",
       "       'AS38794', 'AS133848', 'AS17479', 'AS63940'], dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_th_new[\n",
    "    df_th_new['blocking_recalc'] == 'http.body.confirmed'\n",
    "]['probe_asn'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e8f2ccf0-9644-4c6e-b1d1-3368b9237b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "msmt_counts = df_th_new[\n",
    "    (df_th_new['blocking_recalc'] == 'dns.confirmed') | \n",
    "    (df_th_new['blocking_recalc'] == 'http.title.confirmed') | \n",
    "    (df_th_new['blocking_recalc'] == 'http.body.confirmed')\n",
    "][['input', 'blocking_recalc', 'report_id']].groupby(['input', 'blocking_recalc']).count().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5157460-b535-4108-b6b2-ca48a864a869",
   "metadata": {},
   "source": [
    "And let's check out how many sites were confirmed to be blocked based on our fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f39e1225",
   "metadata": {},
   "outputs": [],
   "source": [
    "msmt_counts.to_csv('2023-thailand-confirmed-by-heuristics')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838a2e30-9065-445c-bc4e-804ea657cb98",
   "metadata": {},
   "source": [
    "We will now reshape the data using `pivot_table` and export the data of `blocking_recalc` into Google Data Studio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b3c2abf1-c2ea-4529-8468-64eea1e83b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_export = df_th_new[['input',\n",
    " 'measurement_start_time',\n",
    " 'probe_asn',\n",
    " 'probe_cc',\n",
    " 'probe_network_name',\n",
    " 'report_id',\n",
    " 'resolver_asn',\n",
    " 'resolver_ip',\n",
    " 'resolver_network_name',\n",
    " 'software_name',\n",
    " 'software_version',\n",
    " 'test_name',\n",
    " 'test_runtime',\n",
    " 'test_version',\n",
    " 'network_type',\n",
    " 'origin',\n",
    " 'platform',\n",
    " 'dns_resolved_ips',\n",
    " 'network_events',\n",
    " 'control_failure',\n",
    " 'control_measurement',\n",
    " 'blocking',\n",
    " 'http_experiment_failure',\n",
    " 'dns_experiment_failure',\n",
    " 'http_title',\n",
    " 'http_meta_title',\n",
    " 'http_body_md5',\n",
    " 'tcp_connect',\n",
    " 'domain',\n",
    " 'explorer_url',\n",
    " 'blocking_recalc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "126aada6-c358-433c-92b9-a63c23c11a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_export['count'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "701e1bf0-27a5-41d8-90c1-6c563f57e59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_export.pivot_table(\n",
    "    index=['input',\n",
    " 'measurement_start_time',\n",
    " 'probe_asn',\n",
    " 'probe_cc',\n",
    " 'probe_network_name',\n",
    " 'report_id',\n",
    " 'resolver_asn',\n",
    " 'resolver_ip',\n",
    " 'resolver_network_name',\n",
    " 'software_name',\n",
    " 'software_version',\n",
    " 'test_name',\n",
    " 'test_runtime',\n",
    " 'test_version',\n",
    " 'network_type',\n",
    " 'origin',\n",
    " 'platform',\n",
    " 'dns_resolved_ips',\n",
    " 'network_events',\n",
    " 'control_failure',\n",
    " 'control_measurement',\n",
    " 'blocking',\n",
    " 'http_experiment_failure',\n",
    " 'dns_experiment_failure',\n",
    " 'http_title',\n",
    " 'http_meta_title',\n",
    " 'http_body_md5',\n",
    " 'tcp_connect',\n",
    " 'domain',\n",
    " 'explorer_url'], \n",
    "    columns=['blocking_recalc'],\n",
    "    values='count'\n",
    ").reset_index().to_csv('2023-thailand-websites.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eebeb35f-4ec9-4d30-b06a-1d2a49f4142a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8142 2023-thailand-websites.csv\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l 2023-thailand-websites.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "903e62f2-9a65-4276-8624-49ad5d339a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19M\t2023-thailand-websites.csv\r\n",
      "19M\ttotal\r\n"
     ]
    }
   ],
   "source": [
    "!du -sch 2023-thailand-websites.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8c730e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
